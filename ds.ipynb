{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T08:19:11.191855Z",
     "start_time": "2024-04-19T08:19:10.546543Z"
    }
   },
   "outputs": [],
   "source": [
    "from data_utils import CQADatasetLoader, SVAMPDatasetLoader, ESNLIDatasetLoader, ANLI1DatasetLoader, ASDivDatasetLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T08:19:11.194458Z",
     "start_time": "2024-04-19T08:19:11.192736Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset_loader = ESNLIDatasetLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T08:19:14.497991Z",
     "start_time": "2024-04-19T08:19:11.195062Z"
    }
   },
   "outputs": [],
   "source": [
    "dataset = dataset_loader.load_from_json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T08:19:15.041900Z",
     "start_time": "2024-04-19T08:19:14.498914Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_llm_rationales, train_llm_labels = dataset_loader.load_llm_preds(split='train')\n",
    "test_llm_rationales, test_llm_labels = dataset_loader.load_llm_preds(split='test')\n",
    "valid_llm_rationales, valid_llm_labels = dataset_loader.load_llm_preds(split='valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T08:19:15.897310Z",
     "start_time": "2024-04-19T08:19:15.043544Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dataset['train'] = dataset['train'].add_column('llm_label', train_llm_labels)\n",
    "dataset['test'] = dataset['test'].add_column('llm_label', test_llm_labels)\n",
    "dataset['train'] = dataset['train'].add_column('llm_rationale', train_llm_rationales)\n",
    "dataset['test'] = dataset['test'].add_column('llm_rationale', test_llm_rationales)\n",
    "dataset['valid'] = dataset['valid'].add_column('llm_label', valid_llm_labels)\n",
    "dataset['valid'] = dataset['valid'].add_column('llm_rationale', valid_llm_rationales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T08:19:19.006459Z",
     "start_time": "2024-04-19T08:19:18.581873Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T08:19:29.766240Z",
     "start_time": "2024-04-19T08:19:19.007115Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "dataset = dataset.map(\n",
    "    lambda example: {'input': tokenizer.eos_token.join([example['premise'], example['hypothesis']])},\n",
    "    remove_columns=['premise', 'hypothesis'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T08:19:29.770832Z",
     "start_time": "2024-04-19T08:19:29.769240Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(examples['input'], max_length=512, truncation=True)\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        rationale_output_encodings = tokenizer(examples['llm_rationale'], max_length=256, truncation=True)\n",
    "\n",
    "    model_inputs['labels'] = rationale_output_encodings['input_ids']\n",
    "    model_inputs['label'] = [label2id[e] for e in examples['label']]\n",
    "\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T08:19:15.899613Z",
     "start_time": "2024-04-19T08:19:15.897881Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "id2label = {0: \"contradiction\", 1: \"entailment\", 2: \"neutral\"}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T08:20:08.688867Z",
     "start_time": "2024-04-19T08:19:29.771362Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "tokenized_datasets = dataset.map(\n",
    "    tokenize_function,\n",
    "    remove_columns=['input', 'llm_label', 'llm_rationale', 'label'],\n",
    "    batched=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenized_datasets.save_to_disk(\"tokenized_datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {0: \"contradiction\", 1: \"entailment\", 2: \"neutral\"}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T08:27:07.864462Z",
     "start_time": "2024-04-19T08:27:06.093634Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of T5ForConditionalGenerationAndSequenceClassification were not initialized from the model checkpoint at google/flan-t5-small and are newly initialized: ['clf_head.bias', 'clf_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from t5_enc.t5 import T5ForConditionalGenerationAndSequenceClassification\n",
    "\n",
    "model = T5ForConditionalGenerationAndSequenceClassification.from_pretrained(\"google/flan-t5-small\", num_labels=3,\n",
    "                                                                            id2label=id2label, label2id=label2id)\n",
    "\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "tokenized_datasets = load_from_disk(\"tokenized_datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T08:20:08.691390Z",
     "start_time": "2024-04-19T08:20:08.689448Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "tokenized_datasets[\"train\"] = tokenized_datasets['train'].train_test_split(test_size=0.1, seed=0)['test']\n",
    "tokenized_datasets[\"test\"] = tokenized_datasets['test'].train_test_split(test_size=0.1, seed=0)['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 54937\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 983\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['label', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 9842\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    remove_unused_columns = False,\n",
    "    evaluation_strategy = 'steps',\n",
    "    eval_steps=500,\n",
    "    save_steps=500,\n",
    "    save_total_limit=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=250,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"test_accuracy\",\n",
    "    gradient_accumulation_steps=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    seed=0,\n",
    "    prediction_loss_only=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T08:20:08.720232Z",
     "start_time": "2024-04-19T08:20:08.718641Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T08:20:08.722356Z",
     "start_time": "2024-04-19T08:20:08.720788Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "p = None\n",
    "def compute_metrics(eval_pred):\n",
    "    global p\n",
    "    predictions, labels = eval_pred\n",
    "    pred = predictions[2]\n",
    "    true = labels[0]\n",
    "    pred_am = pred.argmax(1)\n",
    "    acc = (pred_am == true).mean()\n",
    "    return {'accuracy': acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T08:20:08.724463Z",
     "start_time": "2024-04-19T08:20:08.722836Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer_kwargs = {\n",
    "    'alpha': 0.8,\n",
    "    'model': model,\n",
    "    'args': training_args,\n",
    "    'train_dataset': tokenized_datasets[\"train\"],\n",
    "    'eval_dataset': {'test': tokenized_datasets[\"test\"],},\n",
    "    'data_collator': data_collator,\n",
    "    'tokenizer': tokenizer,\n",
    "    'compute_metrics': compute_metrics,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Config, Seq2SeqTrainer, Trainer\n",
    "# trainer = Trainer(**trainer_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T08:23:10.832539Z",
     "start_time": "2024-04-19T08:23:10.618022Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from t5_enc.t5 import MyTrainer\n",
    "\n",
    "trainer = MyTrainer(**trainer_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# data = data_collator([tokenized_datasets[\"train\"][i] for i in range(1)])\n",
    "# data = data.to('cuda')\n",
    "# model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model(**data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-19T08:25:25.146704Z",
     "start_time": "2024-04-19T08:25:05.161490Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.utils import is_sagemaker_mp_enabled\n",
    "is_sagemaker_mp_enabled()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, logits, labels = trainer.prediction_step(model, data, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.predict([tokenized_datasets[\"train\"][i] for i in range(2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p[0].argmax(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(p[1] == p[0].argmax(1)).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0. проанализировать лосс\n",
    "1. сначала генерация потом клф\n",
    "1.5. по шагам сначала декодер, потом клф\n",
    "2. чередование\n",
    "3. clipping\n",
    "4. 2-stage distillation (gpt-4 -> t5-large (gen) -> t5-small (gen, kl-div + clf)\n",
    "5. 2-головые модели, как складывать лоссы"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
